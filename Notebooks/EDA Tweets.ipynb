{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top_section\"></a>\n",
    "<div align='center'><font size=\"6\" color=\"#000000\"><b>Feature extraction from Dutch tweets: Exploratory Data Analysis & Application of BERT / Frog</b></font></div>\n",
    "<hr>\n",
    "<div align='center'><font size=\"5\" color=\"#000000\">A General Introduction</font></div>\n",
    "<hr>\n",
    "\n",
    "**I think we all know the power of Twitter by now. It's one of the main communications channel for most of the people on earth, we get most of our daily news through our screens via twitter in these days... With the smart phones entering in our lives our newsfeed is immense but there is one problem: How'd we know if we getting real info? What if we want to seperate real situations from the non relevant data? We're lucky! We have NLP models to do heavy work for us to get distilled info. In this notebook we're going to use some basic and common NLP approaches to give us most accurate results...**\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [Getting the Text Data Ready](#getting_the_text_data_ready)\n",
    "    - [Importing Neccesary Packages](#Importing_Neccesary_Packages)\n",
    "    - [Loading the Data](#Loading_the_Data)\n",
    "    - [Cleaning Text](#Cleaning_Text)    \n",
    "* [Visualizing the Data](#Visualizing_the_Data)\n",
    "    - [Target Distribution](#Target_Distribution)\n",
    "    - [Tweet Lengths](#Tweet_Lengths)\n",
    "    - [Word Counts](#Word_Counts)\n",
    "    - [Word Lengths](#Word_Lengths)\n",
    "    - [Most Common Words](#Most_Common_Words)\n",
    "    - [Most Common Bigrams](#Most_Common_Bigrams)\n",
    "    - [Most Common Trigrams](#Most_Common_Trigrams)    \n",
    "* [Some Extra Analysis](#Some_Extra_Analysis)\n",
    "    - [Determining Topics](#Determining_Topics)\n",
    "    - [Word Cloud](#Word_Cloud)\n",
    "    - [Named Entity Recognition](#Named_Entity_Recognition)    \n",
    "* [Building the Bert Model](#Building_the_Model)\n",
    "    - [Getting Things Ready](#Getting_Things_Ready)\n",
    "    - [Tokenization and Formatting the Inputs](#Tokenization_and_Formatting_the_Inputs)\n",
    "    - [Setting the Bert Classification Model](#Setting_the_Bert_Classification_Model)\n",
    "    - [Training and Evaluating](#Training_and_Evaluating)\n",
    "    - [Predicting and Submission](#Predicting_and_Submission)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "## Summary\n",
    "\n",
    "**In this notebook I tried to apply what I learned on NLP. (Well... That's why they called notebooks right?) This notebook includes: Preprocessing the text, visualizing the processed data by several methods like tweet lenghts, word counts, average word lengths, ngrams etc. I especially wanted to clean data before I visualize it, perhaps you should investigate the raw data you got first then move to cleaning in normal cases but I didn't want to pile it on so I went this way. Then I used some more analysis tecniques like Word Clouds, NER's etc. to give us different angles to look from. At the last part we're going to implement BERT model to do tokenization, classification and prediction with using transformers. I hope this notebook helps some of you as others helped me a lot. I'll talk about them in next chapter.**\n",
    "\n",
    "## References:\n",
    "\n",
    "- [BERT Research from  Chris McCormick](https://www.youtube.com/watch?v=FKlPCK1uFrc): It was a great tutorial and helped me a lot through modelling part.\n",
    "- [The Official BERT Paper](https://arxiv.org/pdf/1810.04805.pdf): It's helpful for understanding theory behind it also I used it while finetuning the model\n",
    "- [Transformers Official Page](https://huggingface.co/transformers/quickstart.html): There are some good examples to write your own models.\n",
    "- [Word Cloud GitHub Page](https://github.com/amueller/word_cloud): Again good examples for your word clouds.\n",
    "- [Exploratory Data Analysis for NLP from Shahul Es](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools): Learned a lot about NLP EDA's here.\n",
    "- [NLP | Exploratory Data Analysis of Text Data from Kamil Mysiak](https://towardsdatascience.com/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d): Again this EDA work was great example for me.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Importing_Neccesary_Packages\"></a>\n",
    "\n",
    "## Importing Neccesary Packages\n",
    "\n",
    "### **First we gonna load lots of libraries we are going to use them through our notebook. I tried to sort and group them by the use (divided by #'s) so I hope it's easier to read for you.**\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "_kg_hide-output": true,
    "id": "-iQ4gUDP6r8E",
    "outputId": "caff2324-61f9-4d53-8eab-f999059b0259"
   },
   "outputs": [],
   "source": [
    "# Most basic stuff for EDA and Import\n",
    "from IPython.display import clear_output\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Imports to increase the visibility of the apply function progress and the ability to run functions on multiple cpu cores.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# Core packages for text processing.\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Libraries for text preprocessing.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "stopword_list = stopwords.words('dutch') # + ['amsterdam']\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_md\", exclude=['ner'])\n",
    "# source NER from the same pipeline package as the last component\n",
    "nlp.add_pipe(\"ner\", source=spacy.load(\"nl_core_news_md\"))\n",
    "\n",
    "# insert the entity ruler\n",
    "nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Loading some sklearn packaces for modelling.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Some packages for word clouds and NER.\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "# Core packages for general use throughout the notebook.\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# For customizing our plots.\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "# Setting some options for general use.\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set(font_scale=1.5)\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50\n",
    "pd.set_option('display.width', 1000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#Setting seeds for consistent results.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_split(tup_arg, **kwargs):\n",
    "    split_ind, df_split, df_f_name = tup_arg\n",
    "    return (split_ind, getattr(df_split, df_f_name)(**kwargs))\n",
    "\n",
    "def df_multi_core(df, df_f_name, subset=None, njobs=-1, **kwargs):\n",
    "    if njobs == -1:\n",
    "        njobs = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=njobs)\n",
    "\n",
    "    try:\n",
    "        splits = np.array_split(df[subset], njobs)\n",
    "    except ValueError:\n",
    "        splits = np.array_split(df, njobs)\n",
    "\n",
    "    pool_data = [(split_ind, df_split, df_f_name) for split_ind, df_split in enumerate(splits)]\n",
    "    results = pool.map(partial(_df_split, **kwargs), pool_data)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    results = sorted(results, key=lambda x:x[0])\n",
    "    results = pd.concat([split[1] for split in results])\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Loading_the_Data\"></a>\n",
    "## Loading the Data\n",
    "\n",
    "### **The tweets will be imported from the trollrensics database, based on three values: investigation, begin and end date.**\n",
    "\n",
    "#### After loading the train and test data, I added \"v\"'s at the end of our variables for visualization because some of the pre-processing are not needed for the modelling but we can use them for our EDA part. Anyways let's look at our given data:\n",
    "\n",
    "#### Well... We have keywords, locations, text and our target labels. We gonna use the text feature for our modelling here.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the sql query\n",
    "def create_dataframe(begindate, enddate, investigation):\n",
    "    con = pymysql.connect(\n",
    "        host='127.0.0.1', \n",
    "        user='user',\n",
    "        password='password', \n",
    "        database='trollrensics')\n",
    "\n",
    "    sql = f\"\"\"\n",
    "      select p.platform, p.uniqueid, p.message, p.platform_created_at, p.topic_uniqueid, p.reply_uniqueid, p.share_uniqueid, p.like_cnt, p.reply_cnt, p.quote_cnt, p.share_cnt,\n",
    "          p.trollrensics_metadata,\n",
    "          pr.uniqueid AS profile_uniqueid, pr.username, pr.fullname, pr.followers_cnt, pr.following_cnt, pr.posts_cnt, pr.platform_metadata\n",
    "      from posts p\n",
    "      left join profiles pr on pr.id = p.profile_id\n",
    "      left join investigation_post ip on ip.post_id = p.id\n",
    "      left join investigations i on i.id = ip.investigation_id\n",
    "      where p.platform_created_at > '{begindate}' and p.platform_created_at < '{enddate}'\n",
    "        and i.slug in('{investigation}')\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(sql, con)\n",
    "    df['Investigation'] = investigation\n",
    "    print(df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42241, 20)\n",
      "(22756, 20)\n",
      "(17701, 20)\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "begindate = '2020-09-01'\n",
    "enddate = '2021-05-01'\n",
    "investigation = 'museumplein-druktebeeld'\n",
    "frames.append(create_dataframe(begindate, enddate, investigation))\n",
    "\n",
    "investigation = 'dedam-druktebeeld'\n",
    "frames.append(create_dataframe(begindate, enddate, investigation))\n",
    "\n",
    "investigation = 'vondelpark-druktebeeld'\n",
    "frames.append(create_dataframe(begindate, enddate, investigation))\n",
    "\n",
    "df = pd.concat(frames)\n",
    "df.drop_duplicates(subset='uniqueid', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    78933\n",
       "True      2398\n",
       "Name: message, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df.message.duplicated(keep=False)\n",
    "mask.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 81331 entries, 0 to 81330\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   uniqueid          81331 non-null  object\n",
      " 1   text              81331 non-null  object\n",
      " 2   topic_uniqueid    81331 non-null  object\n",
      " 3   reply_uniqueid    35661 non-null  object\n",
      " 4   share_uniqueid    0 non-null      object\n",
      " 5   like_cnt          81331 non-null  int64 \n",
      " 6   reply_cnt         81331 non-null  int64 \n",
      " 7   quote_cnt         81331 non-null  int64 \n",
      " 8   share_cnt         81331 non-null  int64 \n",
      " 9   profile_uniqueid  81331 non-null  object\n",
      " 10  followers_cnt     81331 non-null  int64 \n",
      " 11  following_cnt     81331 non-null  int64 \n",
      " 12  posts_cnt         81331 non-null  int64 \n",
      " 13  Investigation     81331 non-null  object\n",
      " 14  date              81331 non-null  object\n",
      "dtypes: int64(7), object(8)\n",
      "memory usage: 9.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Transform the date column to remove the time and only show dates\n",
    "df['date'] = pd.to_datetime(df['platform_created_at']).dt.date\n",
    "\n",
    "# df['platform_created_at'] = df['platform_created_at'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Drop the columns that are not needed for this research based on following reasons:\n",
    "# platform: Twitter is the only platform to be considered for this data\n",
    "# trollresnsics_metadata & platform_metadata: both contain a lot of null-values and do not provide usefull information for this research\n",
    "# username & fullname: This research does not need the account names, only the given unique id's. This way the project is also anonymized\n",
    "\n",
    "df.drop(columns=['platform', 'platform_created_at', 'trollrensics_metadata', 'username', 'fullname', 'platform_metadata'], inplace=True)\n",
    "df.rename(columns={\"message\": \"text\"}, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.info()\n",
    "df.to_csv('raw_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Cleaning_Text\"></a>\n",
    "## Cleaning Text\n",
    "\n",
    "#### Before the visualization of the text data I wanted to make it look better with some general helper functions to clear out things like: urls, hashtags, mentions and punctuations... We'll add all of them in one column called 'text_clean' then move from there for next steps. When we have cleaner text we can apply our tokenizer to split each word into a token. I'll apply this and next steps to individual columns to show each step of our progress. Next we transforming all words to lowercase then we remove stopwords (they don't mean much in sentence alone) so we use NLTK stopwords for it.\n",
    "\n",
    "#### After removing these words we gonna lemmatize them but for that we need to add some extra steps to do it properly: We gonna apply part of speech tags to our text (like verb, noun etc.) then we convert them to wordnet format and finally we can apply lemmatizer and save it to 'lemmatized' column. And one last thing we convert these tokenized lists back to str version for future uses.\n",
    "\n",
    "#### So basically what we did are:\n",
    "- Removed urls, emojis, html tags and punctuations,\n",
    "- Tokenized the tweet base texts,\n",
    "- Lower cased clean text,\n",
    "- Removed stopwords,\n",
    "- Applied part of speech tags,\n",
    "- Converted part of speeches to wordnet format,\n",
    "- Applying word lemmatizer,\n",
    "- Converted tokenized text to string again.\n",
    "\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the lemmatized text, pos-tags and named entities for a tweet. \n",
    "def tweet_extraction(row):\n",
    "    doc =nlp(row['only_words'])\n",
    "\n",
    "    data  = [(t.text, t.lemma_, t.pos_, t.lemma_.lower()) for t in doc if not t.is_stop]\n",
    "    # row['spacy_text'], row['lemma'], row['pos']  = [(t.text, t.lemma_, t.pos_) for t in doc if t.lemma_ not in stopword_list]\n",
    "    spacy_text = []\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    lemma_low = []\n",
    "\n",
    "    for x in data:\n",
    "        spacy_text.append(x[0])\n",
    "        lemma.append(x[1])\n",
    "        pos.append(x[2])\n",
    "        lemma_low.append(x[3])\n",
    "    row['spacy_text'] = spacy_text\n",
    "    row['lemma'] = lemma\n",
    "    row['pos'] = pos\n",
    "    row['lemma_low'] = lemma_low\n",
    "    row['ents'] = [(e.text, e.label_) for e in doc.ents]\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_nice(tag):\n",
    "    table = str.maketrans('x', 'x', string.punctuation)\n",
    "    clean_text = tag.translate(table)\n",
    "    new_clean = clean_text.replace(\"\\n\", ' ')\n",
    "    clean = new_clean.split(' ')\n",
    "    clean = clean[0].lower()\n",
    "\n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_uniqueid</th>\n",
       "      <th>reply_uniqueid</th>\n",
       "      <th>share_uniqueid</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>reply_cnt</th>\n",
       "      <th>quote_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>profile_uniqueid</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>following_cnt</th>\n",
       "      <th>posts_cnt</th>\n",
       "      <th>Investigation</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1376795456186032128</td>\n",
       "      <td>@RHvL88 Het is maar een stief kwartiertje lope...</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36447473</td>\n",
       "      <td>29475</td>\n",
       "      <td>1724</td>\n",
       "      <td>212683</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>Amsterdam | #Grachtengordel  #wanddecoratie\\n1...</td>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816668454950490112</td>\n",
       "      <td>89</td>\n",
       "      <td>192</td>\n",
       "      <td>993</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1376801305084198914</td>\n",
       "      <td>@AT5 het museumplein is een hele leuke lokatie...</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227119302</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>1901</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376800177332314113</td>\n",
       "      <td>@rivliv NL leuk oefenveld: vreedzame burgers d...</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2914267607</td>\n",
       "      <td>784</td>\n",
       "      <td>1384</td>\n",
       "      <td>3864</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376792135454883844</td>\n",
       "      <td>Demonstreren op het #museumplein, #klimaatalar...</td>\n",
       "      <td>1376792135454883844</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>244509097</td>\n",
       "      <td>751</td>\n",
       "      <td>867</td>\n",
       "      <td>12495</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1376262673952825348</td>\n",
       "      <td>Zo herkenbaar dit. Ik werd laatst ook geweiger...</td>\n",
       "      <td>1376262673952825348</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>49319243</td>\n",
       "      <td>5360</td>\n",
       "      <td>2173</td>\n",
       "      <td>33478</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1376262413025165319</td>\n",
       "      <td>@MarijnissenL We hebben vaak kritiek op landen...</td>\n",
       "      <td>1376121609820504064</td>\n",
       "      <td>1376121609820504064</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>153776534</td>\n",
       "      <td>11457</td>\n",
       "      <td>12600</td>\n",
       "      <td>497825</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1376262767431319555</td>\n",
       "      <td>@JDTVproducties @drsmichakat Nee hoor, ze wijz...</td>\n",
       "      <td>1376216450977968130</td>\n",
       "      <td>1376261957775425540</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1249328291313483776</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>5421</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1376262741225312258</td>\n",
       "      <td>@lammert We hebben vaak kritiek op landen waar...</td>\n",
       "      <td>1376098406897172480</td>\n",
       "      <td>1376098406897172480</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>153776534</td>\n",
       "      <td>11457</td>\n",
       "      <td>12600</td>\n",
       "      <td>497825</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1376262102348861441</td>\n",
       "      <td>Een stelletje ‘wappies’ worden op het Museumpl...</td>\n",
       "      <td>1376262102348861441</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>72</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>463302756</td>\n",
       "      <td>7278</td>\n",
       "      <td>7254</td>\n",
       "      <td>2844</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 uniqueid                                               text       topic_uniqueid       reply_uniqueid share_uniqueid  like_cnt  reply_cnt  quote_cnt  share_cnt     profile_uniqueid  followers_cnt  following_cnt  posts_cnt            Investigation        date\n",
       "0     1376795456186032128  @RHvL88 Het is maar een stief kwartiertje lope...  1376793859896844291  1376793859896844291           None         1          0          0          0             36447473          29475           1724     212683  museumplein-druktebeeld  2021-03-30\n",
       "1     1376793687477391361  Amsterdam | #Grachtengordel  #wanddecoratie\\n1...  1376793687477391361                 None           None         0          0          0          0   816668454950490112             89            192        993  museumplein-druktebeeld  2021-03-30\n",
       "2     1376801305084198914  @AT5 het museumplein is een hele leuke lokatie...  1376761700091887617  1376761700091887617           None         1          0          0          0            227119302             39             30       1901  museumplein-druktebeeld  2021-03-30\n",
       "3     1376800177332314113  @rivliv NL leuk oefenveld: vreedzame burgers d...  1376656349157851136  1376656349157851136           None         0          0          0          1           2914267607            784           1384       3864  museumplein-druktebeeld  2021-03-30\n",
       "4     1376792135454883844  Demonstreren op het #museumplein, #klimaatalar...  1376792135454883844                 None           None         2          0          0          1            244509097            751            867      12495  museumplein-druktebeeld  2021-03-30\n",
       "...                   ...                                                ...                  ...                  ...            ...       ...        ...        ...        ...                  ...            ...            ...        ...                      ...         ...\n",
       "1995  1376262673952825348  Zo herkenbaar dit. Ik werd laatst ook geweiger...  1376262673952825348                 None           None        15          2          1          2             49319243           5360           2173      33478  museumplein-druktebeeld  2021-03-28\n",
       "1996  1376262413025165319  @MarijnissenL We hebben vaak kritiek op landen...  1376121609820504064  1376121609820504064           None        16          3          0          7            153776534          11457          12600     497825  museumplein-druktebeeld  2021-03-28\n",
       "1997  1376262767431319555  @JDTVproducties @drsmichakat Nee hoor, ze wijz...  1376216450977968130  1376261957775425540           None         1          0          0          0  1249328291313483776             45             11       5421  museumplein-druktebeeld  2021-03-28\n",
       "1998  1376262741225312258  @lammert We hebben vaak kritiek op landen waar...  1376098406897172480  1376098406897172480           None         4          1          1          0            153776534          11457          12600     497825  museumplein-druktebeeld  2021-03-28\n",
       "1999  1376262102348861441  Een stelletje ‘wappies’ worden op het Museumpl...  1376262102348861441                 None           None        72          8          2         16            463302756           7278           7254       2844  museumplein-druktebeeld  2021-03-28\n",
       "\n",
       "[2000 rows x 15 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is for testing only\n",
    "\n",
    "df_test = df.iloc[:2000]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "UDcSNAo86r8U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:11<00:00, 21.36it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 20.23it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 19.97it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 19.95it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 19.68it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 19.51it/s]\n",
      "100%|██████████| 250/250 [00:12<00:00, 19.32it/s]\n",
      "100%|██████████| 250/250 [00:13<00:00, 18.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_uniqueid</th>\n",
       "      <th>reply_uniqueid</th>\n",
       "      <th>share_uniqueid</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>reply_cnt</th>\n",
       "      <th>quote_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>profile_uniqueid</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>following_cnt</th>\n",
       "      <th>posts_cnt</th>\n",
       "      <th>Investigation</th>\n",
       "      <th>date</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>spacy_text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma_low</th>\n",
       "      <th>ents</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>len_mentions</th>\n",
       "      <th>len_hashtags</th>\n",
       "      <th>len_urls</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1376795456186032128</td>\n",
       "      <td>@RHvL88 Het is maar een stief kwartiertje lope...</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36447473</td>\n",
       "      <td>29475</td>\n",
       "      <td>1724</td>\n",
       "      <td>212683</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>RHvL88 Het is maar een stief kwartiertje lopen...</td>\n",
       "      <td>[RHvL88, stief, kwartiertje, lopen, Rokin, Mus...</td>\n",
       "      <td>[rhvl88, stief, kwartier, lopen, rokin, museum...</td>\n",
       "      <td>[PROPN, NOUN, NOUN, VERB, NOUN, PROPN]</td>\n",
       "      <td>[rhvl88, stief, kwartier, lopen, rokin, museum...</td>\n",
       "      <td>[(Rokin, LOC), (Museumplei, LOC)]</td>\n",
       "      <td>[rhvl88]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rhvl88 het is maar een stief kwartiertje lopen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>Amsterdam | #Grachtengordel  #wanddecoratie\\n1...</td>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816668454950490112</td>\n",
       "      <td>89</td>\n",
       "      <td>192</td>\n",
       "      <td>993</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>Amsterdam  Grachtengordel  wanddecoratie\\n10 L...</td>\n",
       "      <td>[Amsterdam,  , Grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[amsterdam,  , grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[PROPN, SPACE, NOUN, SPACE, NOUN, SPACE, NUM, ...</td>\n",
       "      <td>[amsterdam,  , grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[(Amsterdam, GPE), (Grachtengordel, FAC), (10,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[grachtengordel, wanddecoratie, lentekorting, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amsterdam  grachtengordel  wanddecoratie\\n10 l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1376194425102209029</td>\n",
       "      <td>Amsterdam | #Grachtengordel  #wanddecoratie\\n1...</td>\n",
       "      <td>1376194425102209029</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816668454950490112</td>\n",
       "      <td>89</td>\n",
       "      <td>192</td>\n",
       "      <td>993</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>Amsterdam  Grachtengordel  wanddecoratie\\n10 L...</td>\n",
       "      <td>[Amsterdam,  , Grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[amsterdam,  , grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[PROPN, SPACE, NOUN, SPACE, NOUN, SPACE, NUM, ...</td>\n",
       "      <td>[amsterdam,  , grachtengordel,  , wanddecorati...</td>\n",
       "      <td>[(Amsterdam, GPE), (Grachtengordel, FAC), (10,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[grachtengordel, wanddecoratie, lentekorting, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amsterdam  grachtengordel  wanddecoratie\\n10 l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376801305084198914</td>\n",
       "      <td>@AT5 het museumplein is een hele leuke lokatie...</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227119302</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>1901</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>AT5 het museumplein is een hele leuke lokatie ...</td>\n",
       "      <td>[AT5, museumplein, leuke, lokatie,  , feesten,...</td>\n",
       "      <td>[at5, museumplein, leuk, lokatie,  , feesten, ...</td>\n",
       "      <td>[PROPN, NOUN, ADJ, NOUN, SPACE, VERB, NOUN, VERB]</td>\n",
       "      <td>[at5, museumplein, leuk, lokatie,  , feesten, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[at5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>at5 het museumplein is een hele leuke lokatie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376800177332314113</td>\n",
       "      <td>@rivliv NL leuk oefenveld: vreedzame burgers d...</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2914267607</td>\n",
       "      <td>784</td>\n",
       "      <td>1384</td>\n",
       "      <td>3864</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>rivliv NL leuk oefenveld vreedzame burgers daa...</td>\n",
       "      <td>[rivliv, NL, leuk, oefenveld, vreedzame, burge...</td>\n",
       "      <td>[rivliv, nl, leuk, oefenveld, vreedzaam, burge...</td>\n",
       "      <td>[NOUN, PROPN, ADJ, VERB, ADJ, NOUN, VERB, VERB...</td>\n",
       "      <td>[rivliv, nl, leuk, oefenveld, vreedzaam, burge...</td>\n",
       "      <td>[(NL, NORP), (Kijk die generaals op het museum...</td>\n",
       "      <td>[rivliv]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rivliv nl leuk oefenveld vreedzame burgers daa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              uniqueid                                               text       topic_uniqueid       reply_uniqueid share_uniqueid  like_cnt  reply_cnt  quote_cnt  share_cnt    profile_uniqueid  followers_cnt  following_cnt  posts_cnt            Investigation        date                                         clean_text                                         spacy_text                                              lemma                                                pos                                          lemma_low                                               ents  mentions                                           hashtags urls  len_mentions  len_hashtags  len_urls                                              lower\n",
       "0  1376795456186032128  @RHvL88 Het is maar een stief kwartiertje lope...  1376793859896844291  1376793859896844291           None         1          0          0          0            36447473          29475           1724     212683  museumplein-druktebeeld  2021-03-30  RHvL88 Het is maar een stief kwartiertje lopen...  [RHvL88, stief, kwartiertje, lopen, Rokin, Mus...  [rhvl88, stief, kwartier, lopen, rokin, museum...             [PROPN, NOUN, NOUN, VERB, NOUN, PROPN]  [rhvl88, stief, kwartier, lopen, rokin, museum...                  [(Rokin, LOC), (Museumplei, LOC)]  [rhvl88]                                                 []   []           1.0           0.0       0.0  rhvl88 het is maar een stief kwartiertje lopen...\n",
       "1  1376793687477391361  Amsterdam | #Grachtengordel  #wanddecoratie\\n1...  1376793687477391361                 None           None         0          0          0          0  816668454950490112             89            192        993  museumplein-druktebeeld  2021-03-30  Amsterdam  Grachtengordel  wanddecoratie\\n10 L...  [Amsterdam,  , Grachtengordel,  , wanddecorati...  [amsterdam,  , grachtengordel,  , wanddecorati...  [PROPN, SPACE, NOUN, SPACE, NOUN, SPACE, NUM, ...  [amsterdam,  , grachtengordel,  , wanddecorati...  [(Amsterdam, GPE), (Grachtengordel, FAC), (10,...        []  [grachtengordel, wanddecoratie, lentekorting, ...   []           0.0          17.0       0.0  amsterdam  grachtengordel  wanddecoratie\\n10 l...\n",
       "2  1376194425102209029  Amsterdam | #Grachtengordel  #wanddecoratie\\n1...  1376194425102209029                 None           None         1          0          0          0  816668454950490112             89            192        993  museumplein-druktebeeld  2021-03-28  Amsterdam  Grachtengordel  wanddecoratie\\n10 L...  [Amsterdam,  , Grachtengordel,  , wanddecorati...  [amsterdam,  , grachtengordel,  , wanddecorati...  [PROPN, SPACE, NOUN, SPACE, NOUN, SPACE, NUM, ...  [amsterdam,  , grachtengordel,  , wanddecorati...  [(Amsterdam, GPE), (Grachtengordel, FAC), (10,...        []  [grachtengordel, wanddecoratie, lentekorting, ...   []           0.0          17.0       0.0  amsterdam  grachtengordel  wanddecoratie\\n10 l...\n",
       "3  1376801305084198914  @AT5 het museumplein is een hele leuke lokatie...  1376761700091887617  1376761700091887617           None         1          0          0          0           227119302             39             30       1901  museumplein-druktebeeld  2021-03-30  AT5 het museumplein is een hele leuke lokatie ...  [AT5, museumplein, leuke, lokatie,  , feesten,...  [at5, museumplein, leuk, lokatie,  , feesten, ...  [PROPN, NOUN, ADJ, NOUN, SPACE, VERB, NOUN, VERB]  [at5, museumplein, leuk, lokatie,  , feesten, ...                                                 []     [at5]                                                 []   []           1.0           0.0       0.0  at5 het museumplein is een hele leuke lokatie ...\n",
       "4  1376800177332314113  @rivliv NL leuk oefenveld: vreedzame burgers d...  1376656349157851136  1376656349157851136           None         0          0          0          1          2914267607            784           1384       3864  museumplein-druktebeeld  2021-03-30  rivliv NL leuk oefenveld vreedzame burgers daa...  [rivliv, NL, leuk, oefenveld, vreedzame, burge...  [rivliv, nl, leuk, oefenveld, vreedzaam, burge...  [NOUN, PROPN, ADJ, VERB, ADJ, NOUN, VERB, VERB...  [rivliv, nl, leuk, oefenveld, vreedzaam, burge...  [(NL, NORP), (Kijk die generaals op het museum...  [rivliv]                                                 []   []           1.0           0.0       0.0  rivliv nl leuk oefenveld vreedzame burgers daa..."
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n",
    "SYMBOLS = ['~', ':', \"'\", '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/']\n",
    "\n",
    "def clean_all(row):\n",
    "    text = row['text']\n",
    "    mentions = []\n",
    "    hashtags = []\n",
    "    urls = []\n",
    "    only_words = ''\n",
    "    only_words_lower = ''\n",
    "    words = text.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        if word.startswith('@'):\n",
    "            cleaned = make_hashtag_nice(word)\n",
    "            mentions.append(cleaned)\n",
    "            temp_word = f'mention{cleaned}'\n",
    "            only_words += f'{temp_word} '\n",
    "        elif word.startswith('#'):\n",
    "            cleaned = make_hashtag_nice(word)\n",
    "            hashtags.append(cleaned)\n",
    "            temp_word = f'hastag{cleaned}'\n",
    "            only_words += f'{temp_word} '\n",
    "        elif word.startswith('https?://') or word.startswith('www\\.'):\n",
    "            urls.append(word)\n",
    "        else:\n",
    "            if word not in SYMBOLS:\n",
    "                only_words += f'{word} '\n",
    "\n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     clean_text = text.translate(table)\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', only_words)\n",
    "    clean_text = clean_text.strip('\\\\n')\n",
    "    row['only_words'] = clean_text\n",
    "    row = tweet_extraction(row)\n",
    "\n",
    "    row['mentions'] = mentions\n",
    "    row['hashtags'] = hashtags\n",
    "    row['urls'] = urls\n",
    "    row['len_mentions'] = len(mentions)\n",
    "    row['len_hashtags'] = len(hashtags)\n",
    "    row['len_urls'] = len(urls)\n",
    "    row['lower'] = clean_text.lower()\n",
    "    return row\n",
    "\n",
    "df_text = df_multi_core(df=df_test, df_f_name='progress_apply', subset=['text'],njobs=-1, func=clean_all, axis=1)\n",
    "\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_uniqueid</th>\n",
       "      <th>reply_uniqueid</th>\n",
       "      <th>share_uniqueid</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>reply_cnt</th>\n",
       "      <th>quote_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>profile_uniqueid</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>following_cnt</th>\n",
       "      <th>posts_cnt</th>\n",
       "      <th>Investigation</th>\n",
       "      <th>date</th>\n",
       "      <th>only_words</th>\n",
       "      <th>spacy_text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma_low</th>\n",
       "      <th>ents</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>len_mentions</th>\n",
       "      <th>len_hashtags</th>\n",
       "      <th>len_urls</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1376795456186032128</td>\n",
       "      <td>@RHvL88 Het is maar een stief kwartiertje lope...</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>1376793859896844291</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36447473</td>\n",
       "      <td>29475</td>\n",
       "      <td>1724</td>\n",
       "      <td>212683</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>mentionrhvl88 Het is maar een stief kwartiertj...</td>\n",
       "      <td>[mentionrhvl88, stief, kwartiertje, lopen, Rok...</td>\n",
       "      <td>[mentionrhvl88, stief, kwartier, lopen, rokin,...</td>\n",
       "      <td>[NOUN, NOUN, NOUN, VERB, PROPN, PROPN]</td>\n",
       "      <td>[mentionrhvl88, stief, kwartier, lopen, rokin,...</td>\n",
       "      <td>[(Rokin, LOC), (Museumplein, LOC)]</td>\n",
       "      <td>[rhvl88]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mentionrhvl88 het is maar een stief kwartiertj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>Amsterdam | #Grachtengordel  #wanddecoratie\\n1...</td>\n",
       "      <td>1376793687477391361</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816668454950490112</td>\n",
       "      <td>89</td>\n",
       "      <td>192</td>\n",
       "      <td>993</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>Amsterdam hastaggrachtengordel  hastagwanddeco...</td>\n",
       "      <td>[Amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[PROPN, NOUN, SPACE, NOUN, NOUN, NOUN, ADP, NU...</td>\n",
       "      <td>[amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[(Amsterdam, GPE), (5 april 2021, DATE), (Amst...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[grachtengordel, wanddecoratie, lentekorting, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amsterdam hastaggrachtengordel  hastagwanddeco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1376194425102209029</td>\n",
       "      <td>Amsterdam | #Grachtengordel  #wanddecoratie\\n1...</td>\n",
       "      <td>1376194425102209029</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>816668454950490112</td>\n",
       "      <td>89</td>\n",
       "      <td>192</td>\n",
       "      <td>993</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-28</td>\n",
       "      <td>Amsterdam hastaggrachtengordel  hastagwanddeco...</td>\n",
       "      <td>[Amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[PROPN, NOUN, SPACE, NOUN, NOUN, NOUN, ADP, NU...</td>\n",
       "      <td>[amsterdam, hastaggrachtengordel,  , hastagwan...</td>\n",
       "      <td>[(Amsterdam, GPE), (5 april 2021, DATE), (Amst...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[grachtengordel, wanddecoratie, lentekorting, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>amsterdam hastaggrachtengordel  hastagwanddeco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1376801305084198914</td>\n",
       "      <td>@AT5 het museumplein is een hele leuke lokatie...</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>1376761700091887617</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227119302</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>1901</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>mentionat5 het museumplein is een hele leuke l...</td>\n",
       "      <td>[mentionat5, museumplein, leuke, lokatie,  , f...</td>\n",
       "      <td>[mentionat5, museumplein, leuk, lokatie,  , fe...</td>\n",
       "      <td>[NOUN, NOUN, ADJ, NOUN, SPACE, VERB, NOUN, VERB]</td>\n",
       "      <td>[mentionat5, museumplein, leuk, lokatie,  , fe...</td>\n",
       "      <td>[(mentionat5, PRODUCT)]</td>\n",
       "      <td>[at5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mentionat5 het museumplein is een hele leuke l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1376800177332314113</td>\n",
       "      <td>@rivliv NL leuk oefenveld: vreedzame burgers d...</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>1376656349157851136</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2914267607</td>\n",
       "      <td>784</td>\n",
       "      <td>1384</td>\n",
       "      <td>3864</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-30</td>\n",
       "      <td>mentionrivliv NL leuk oefenveld vreedzame burg...</td>\n",
       "      <td>[mentionrivliv, NL, leuk, oefenveld, vreedzame...</td>\n",
       "      <td>[mentionrivliv, nl, leuk, oefenveld, vreedzaam...</td>\n",
       "      <td>[PROPN, PROPN, ADJ, VERB, ADJ, NOUN, VERB, VER...</td>\n",
       "      <td>[mentionrivliv, nl, leuk, oefenveld, vreedzaam...</td>\n",
       "      <td>[(NL, NORP), (Kijk die generaals op het museum...</td>\n",
       "      <td>[rivliv]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mentionrivliv nl leuk oefenveld vreedzame burg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              uniqueid                                               text       topic_uniqueid       reply_uniqueid share_uniqueid  like_cnt  reply_cnt  quote_cnt  share_cnt    profile_uniqueid  followers_cnt  following_cnt  posts_cnt            Investigation        date                                         only_words                                         spacy_text                                              lemma                                                pos                                          lemma_low                                               ents  mentions                                           hashtags urls  len_mentions  len_hashtags  len_urls                                              lower\n",
       "0  1376795456186032128  @RHvL88 Het is maar een stief kwartiertje lope...  1376793859896844291  1376793859896844291           None         1          0          0          0            36447473          29475           1724     212683  museumplein-druktebeeld  2021-03-30  mentionrhvl88 Het is maar een stief kwartiertj...  [mentionrhvl88, stief, kwartiertje, lopen, Rok...  [mentionrhvl88, stief, kwartier, lopen, rokin,...             [NOUN, NOUN, NOUN, VERB, PROPN, PROPN]  [mentionrhvl88, stief, kwartier, lopen, rokin,...                 [(Rokin, LOC), (Museumplein, LOC)]  [rhvl88]                                                 []   []           1.0           0.0       0.0  mentionrhvl88 het is maar een stief kwartiertj...\n",
       "1  1376793687477391361  Amsterdam | #Grachtengordel  #wanddecoratie\\n1...  1376793687477391361                 None           None         0          0          0          0  816668454950490112             89            192        993  museumplein-druktebeeld  2021-03-30  Amsterdam hastaggrachtengordel  hastagwanddeco...  [Amsterdam, hastaggrachtengordel,  , hastagwan...  [amsterdam, hastaggrachtengordel,  , hastagwan...  [PROPN, NOUN, SPACE, NOUN, NOUN, NOUN, ADP, NU...  [amsterdam, hastaggrachtengordel,  , hastagwan...  [(Amsterdam, GPE), (5 april 2021, DATE), (Amst...        []  [grachtengordel, wanddecoratie, lentekorting, ...   []           0.0          17.0       0.0  amsterdam hastaggrachtengordel  hastagwanddeco...\n",
       "2  1376194425102209029  Amsterdam | #Grachtengordel  #wanddecoratie\\n1...  1376194425102209029                 None           None         1          0          0          0  816668454950490112             89            192        993  museumplein-druktebeeld  2021-03-28  Amsterdam hastaggrachtengordel  hastagwanddeco...  [Amsterdam, hastaggrachtengordel,  , hastagwan...  [amsterdam, hastaggrachtengordel,  , hastagwan...  [PROPN, NOUN, SPACE, NOUN, NOUN, NOUN, ADP, NU...  [amsterdam, hastaggrachtengordel,  , hastagwan...  [(Amsterdam, GPE), (5 april 2021, DATE), (Amst...        []  [grachtengordel, wanddecoratie, lentekorting, ...   []           0.0          17.0       0.0  amsterdam hastaggrachtengordel  hastagwanddeco...\n",
       "3  1376801305084198914  @AT5 het museumplein is een hele leuke lokatie...  1376761700091887617  1376761700091887617           None         1          0          0          0           227119302             39             30       1901  museumplein-druktebeeld  2021-03-30  mentionat5 het museumplein is een hele leuke l...  [mentionat5, museumplein, leuke, lokatie,  , f...  [mentionat5, museumplein, leuk, lokatie,  , fe...   [NOUN, NOUN, ADJ, NOUN, SPACE, VERB, NOUN, VERB]  [mentionat5, museumplein, leuk, lokatie,  , fe...                            [(mentionat5, PRODUCT)]     [at5]                                                 []   []           1.0           0.0       0.0  mentionat5 het museumplein is een hele leuke l...\n",
       "4  1376800177332314113  @rivliv NL leuk oefenveld: vreedzame burgers d...  1376656349157851136  1376656349157851136           None         0          0          0          1          2914267607            784           1384       3864  museumplein-druktebeeld  2021-03-30  mentionrivliv NL leuk oefenveld vreedzame burg...  [mentionrivliv, NL, leuk, oefenveld, vreedzame...  [mentionrivliv, nl, leuk, oefenveld, vreedzaam...  [PROPN, PROPN, ADJ, VERB, ADJ, NOUN, VERB, VER...  [mentionrivliv, nl, leuk, oefenveld, vreedzaam...  [(NL, NORP), (Kijk die generaals op het museum...  [rivliv]                                                 []   []           1.0           0.0       0.0  mentionrivliv nl leuk oefenveld vreedzame burg..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = df.merge(df_text, how='outer')\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import datetime as dt'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82405, 28)\n",
      "(74106, 28)\n",
      "(8299, 28)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes into 90% for training (80%) plus validation (10%) purposes and 10% for testing\n",
    "\n",
    "mask = total_df['date'] <= dt.date(2021,4,8)\n",
    "trainv = total_df.loc[mask]\n",
    "testv = total_df.drop(trainv.index)\n",
    "\n",
    "trainv.reset_index(drop=True)\n",
    "testv.reset_index(drop=True)\n",
    "print(total_df.shape)\n",
    "print(trainv.shape)\n",
    "print(testv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_uniqueid</th>\n",
       "      <th>reply_uniqueid</th>\n",
       "      <th>share_uniqueid</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>reply_cnt</th>\n",
       "      <th>quote_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>profile_uniqueid</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>following_cnt</th>\n",
       "      <th>posts_cnt</th>\n",
       "      <th>Investigation</th>\n",
       "      <th>date</th>\n",
       "      <th>only_words</th>\n",
       "      <th>spacy_text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma_low</th>\n",
       "      <th>ents</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>len_mentions</th>\n",
       "      <th>len_hashtags</th>\n",
       "      <th>len_urls</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11912</th>\n",
       "      <td>1373651057612623874</td>\n",
       "      <td>Vinden jullie Nederland ook zo'n TERINGBENDE? ...</td>\n",
       "      <td>1373651057612623874</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1308147614475603973</td>\n",
       "      <td>16</td>\n",
       "      <td>168</td>\n",
       "      <td>83</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30559</th>\n",
       "      <td>1379165428413566977</td>\n",
       "      <td>Zo dan verhuist, mooi plekje kijkend op het mu...</td>\n",
       "      <td>1379165428413566977</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1310841797258760192</td>\n",
       "      <td>376</td>\n",
       "      <td>271</td>\n",
       "      <td>941</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37977</th>\n",
       "      <td>1358361573539139584</td>\n",
       "      <td>BM Halsema gooit Museumplein weer op slot http...</td>\n",
       "      <td>1358361573539139584</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>801534350558892032</td>\n",
       "      <td>2920</td>\n",
       "      <td>3120</td>\n",
       "      <td>249580</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-02-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>1376555419183820801</td>\n",
       "      <td>@__rabbithole__ Dank je.\\n\\nFeestvieren op het...</td>\n",
       "      <td>1376241143260794886</td>\n",
       "      <td>1376389438603943936</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>823336063</td>\n",
       "      <td>851</td>\n",
       "      <td>399</td>\n",
       "      <td>18730</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>mentionrabbithole Dank je\\n\\nFeestvieren op he...</td>\n",
       "      <td>[mentionrabbithole, Dank, \\n\\n, Feestvieren, M...</td>\n",
       "      <td>[mentionrabbithole, danken, \\n\\n, feestvieren,...</td>\n",
       "      <td>[NOUN, VERB, SPACE, PROPN, NOUN, VERB, VERB, S...</td>\n",
       "      <td>[mentionrabbithole, danken, \\n\\n, feestvieren,...</td>\n",
       "      <td>[(Dank je, ORG), (Museumplein, FAC), (Bang ben...</td>\n",
       "      <td>[rabbithole]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mentionrabbithole dank je\\n\\nfeestvieren op he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17404</th>\n",
       "      <td>1373276329982976001</td>\n",
       "      <td>De 'maar weinig doden' zijn resultaat van #cor...</td>\n",
       "      <td>1373276329982976001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>759922975198117888</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>13361</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-03-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  uniqueid                                               text       topic_uniqueid       reply_uniqueid share_uniqueid  like_cnt  reply_cnt  quote_cnt  share_cnt     profile_uniqueid  followers_cnt  following_cnt  posts_cnt            Investigation        date                                         only_words                                         spacy_text                                              lemma                                                pos                                          lemma_low                                               ents      mentions hashtags urls  len_mentions  len_hashtags  len_urls                                              lower\n",
       "11912  1373651057612623874  Vinden jullie Nederland ook zo'n TERINGBENDE? ...  1373651057612623874                 None           None         2          1          0          0  1308147614475603973             16            168         83  museumplein-druktebeeld  2021-03-21                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN           NaN      NaN  NaN           NaN           NaN       NaN                                                NaN\n",
       "30559  1379165428413566977  Zo dan verhuist, mooi plekje kijkend op het mu...  1379165428413566977                 None           None         6          0          0          0  1310841797258760192            376            271        941  museumplein-druktebeeld  2021-04-05                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN           NaN      NaN  NaN           NaN           NaN       NaN                                                NaN\n",
       "37977  1358361573539139584  BM Halsema gooit Museumplein weer op slot http...  1358361573539139584                 None           None         1          0          0          2   801534350558892032           2920           3120     249580  museumplein-druktebeeld  2021-02-07                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN           NaN      NaN  NaN           NaN           NaN       NaN                                                NaN\n",
       "1571   1376555419183820801  @__rabbithole__ Dank je.\\n\\nFeestvieren op het...  1376241143260794886  1376389438603943936           None         1          0          0          0            823336063            851            399      18730  museumplein-druktebeeld  2021-03-29  mentionrabbithole Dank je\\n\\nFeestvieren op he...  [mentionrabbithole, Dank, \\n\\n, Feestvieren, M...  [mentionrabbithole, danken, \\n\\n, feestvieren,...  [NOUN, VERB, SPACE, PROPN, NOUN, VERB, VERB, S...  [mentionrabbithole, danken, \\n\\n, feestvieren,...  [(Dank je, ORG), (Museumplein, FAC), (Bang ben...  [rabbithole]       []   []           1.0           0.0       0.0  mentionrabbithole dank je\\n\\nfeestvieren op he...\n",
       "17404  1373276329982976001  De 'maar weinig doden' zijn resultaat van #cor...  1373276329982976001                 None           None         0          2          1          0   759922975198117888            114              3      13361  museumplein-druktebeeld  2021-03-20                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN                                                NaN           NaN      NaN  NaN           NaN           NaN       NaN                                                NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueid</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_uniqueid</th>\n",
       "      <th>reply_uniqueid</th>\n",
       "      <th>share_uniqueid</th>\n",
       "      <th>like_cnt</th>\n",
       "      <th>reply_cnt</th>\n",
       "      <th>quote_cnt</th>\n",
       "      <th>share_cnt</th>\n",
       "      <th>profile_uniqueid</th>\n",
       "      <th>followers_cnt</th>\n",
       "      <th>following_cnt</th>\n",
       "      <th>posts_cnt</th>\n",
       "      <th>Investigation</th>\n",
       "      <th>date</th>\n",
       "      <th>only_words</th>\n",
       "      <th>spacy_text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>lemma_low</th>\n",
       "      <th>ents</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>len_mentions</th>\n",
       "      <th>len_hashtags</th>\n",
       "      <th>len_urls</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40249</th>\n",
       "      <td>1385936691739963392</td>\n",
       "      <td>@Monte_Lanaco @tweetlelidy Diepe zucht. Dat ze...</td>\n",
       "      <td>1385665061528743944</td>\n",
       "      <td>1385845656519155712</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1033709287107977217</td>\n",
       "      <td>4838</td>\n",
       "      <td>3964</td>\n",
       "      <td>57443</td>\n",
       "      <td>museumplein-druktebeeld</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63725</th>\n",
       "      <td>1386390662053040130</td>\n",
       "      <td>Je kunt tegen het coronabeleid zijn maar je be...</td>\n",
       "      <td>1386390662053040130</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3156576852</td>\n",
       "      <td>1991</td>\n",
       "      <td>1915</td>\n",
       "      <td>104963</td>\n",
       "      <td>dedam-druktebeeld</td>\n",
       "      <td>2021-04-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81006</th>\n",
       "      <td>1387028831081533441</td>\n",
       "      <td>Lieve Heer, kunt u misschien even een plaatsel...</td>\n",
       "      <td>1387028831081533441</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1312427609486241792</td>\n",
       "      <td>1481</td>\n",
       "      <td>712</td>\n",
       "      <td>8229</td>\n",
       "      <td>vondelpark-druktebeeld</td>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52826</th>\n",
       "      <td>1383596479034445835</td>\n",
       "      <td>Se acuerdan en el velatorio de DAM les dieron ...</td>\n",
       "      <td>1383596479034445835</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163992090</td>\n",
       "      <td>363</td>\n",
       "      <td>803</td>\n",
       "      <td>46382</td>\n",
       "      <td>dedam-druktebeeld</td>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52758</th>\n",
       "      <td>1383301071603310594</td>\n",
       "      <td>@Steven_VdB80 Als A onzinnig vindt dat terrass...</td>\n",
       "      <td>1382768561127616517</td>\n",
       "      <td>1383299650581504026</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1179754549256556545</td>\n",
       "      <td>360</td>\n",
       "      <td>865</td>\n",
       "      <td>2334</td>\n",
       "      <td>dedam-druktebeeld</td>\n",
       "      <td>2021-04-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  uniqueid                                               text       topic_uniqueid       reply_uniqueid share_uniqueid  like_cnt  reply_cnt  quote_cnt  share_cnt     profile_uniqueid  followers_cnt  following_cnt  posts_cnt            Investigation        date only_words spacy_text lemma  pos lemma_low ents mentions hashtags urls  len_mentions  len_hashtags  len_urls lower\n",
       "40249  1385936691739963392  @Monte_Lanaco @tweetlelidy Diepe zucht. Dat ze...  1385665061528743944  1385845656519155712           None         3          0          0          0  1033709287107977217           4838           3964      57443  museumplein-druktebeeld  2021-04-24        NaN        NaN   NaN  NaN       NaN  NaN      NaN      NaN  NaN           NaN           NaN       NaN   NaN\n",
       "63725  1386390662053040130  Je kunt tegen het coronabeleid zijn maar je be...  1386390662053040130                 None           None         3          1          0          1           3156576852           1991           1915     104963        dedam-druktebeeld  2021-04-25        NaN        NaN   NaN  NaN       NaN  NaN      NaN      NaN  NaN           NaN           NaN       NaN   NaN\n",
       "81006  1387028831081533441  Lieve Heer, kunt u misschien even een plaatsel...  1387028831081533441                 None           None        52          6          1          8  1312427609486241792           1481            712       8229   vondelpark-druktebeeld  2021-04-27        NaN        NaN   NaN  NaN       NaN  NaN      NaN      NaN  NaN           NaN           NaN       NaN   NaN\n",
       "52826  1383596479034445835  Se acuerdan en el velatorio de DAM les dieron ...  1383596479034445835                 None           None         1          0          0          0            163992090            363            803      46382        dedam-druktebeeld  2021-04-18        NaN        NaN   NaN  NaN       NaN  NaN      NaN      NaN  NaN           NaN           NaN       NaN   NaN\n",
       "52758  1383301071603310594  @Steven_VdB80 Als A onzinnig vindt dat terrass...  1382768561127616517  1383299650581504026           None         0          0          0          0  1179754549256556545            360            865       2334        dedam-druktebeeld  2021-04-17        NaN        NaN   NaN  NaN       NaN  NaN      NaN      NaN  NaN           NaN           NaN       NaN   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking general look at the both datasets.\n",
    "# trainv.to_csv('tweet_csv_train.csv')\n",
    "# testv.to_csv('tweet_csv_test.csv')\n",
    "total_df.to_csv('temporary_df_groot.csv')\n",
    "display(trainv.sample(5))\n",
    "display(testv.sample(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Visualizing_the_Data\"></a>\n",
    "# Visualizing the Data\n",
    "\n",
    "#### Well... Our text is ready for inspection, clean and in order. We can start visualizing the data to see if we can find some visible relations between tweet classes.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Target_Distribution\"></a>\n",
    "## Target Distribution\n",
    "\n",
    "#### When we check our target variables and look at how they disturbuted we can say it not bad. There is no huge difference between classes we can say it's good sign for modelling.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JtW8gflY6r81",
    "outputId": "0124af73-d1c8-4143-d0bc-e408d01d2ae9"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'crowded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crowded'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-65871a9ea187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crowded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m axes[1].pie(trainv['crowded'].value_counts(),\n\u001b[1;32m      6\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No event'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Event'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'crowded'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying target distribution.\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\n",
    "sns.countplot(trainv['crowded'], ax=axes[0])\n",
    "axes[1].pie(trainv['crowded'].value_counts(),\n",
    "            labels=['No event', 'Event'],\n",
    "            autopct='%1.2f%%',\n",
    "            shadow=True,\n",
    "            explode=(0.05, 0),\n",
    "            startangle=60)\n",
    "fig.suptitle('Distribution of the Tweets', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Tweet_Lengths\"></a>\n",
    "## Tweet Lengths\n",
    "\n",
    "#### Let's start with the number of characters per tweet and compare if it's disaster related or not. It seems disaster tweets are longer than non disaster tweets in general. We can assume longer tweets are more likely for disasters but this is only an assumption and might be not true...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "5uEl71g36r86"
   },
   "outputs": [],
   "source": [
    "# Creating a new feature for the visualization.\n",
    "\n",
    "trainv['Character Count'] = trainv['text_clean'].apply(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "def plot_dist3(df, feature, title):\n",
    "    # Creating a customized chart. and giving in figsize and everything.\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n",
    "    # Creating a grid of 3 cols and 3 rows.\n",
    "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "\n",
    "    # Customizing the histogram grid.\n",
    "    ax1 = fig.add_subplot(grid[0, :2])\n",
    "    # Set the title.\n",
    "    ax1.set_title('Histogram')\n",
    "    # plot the histogram.\n",
    "    sns.distplot(df.loc[:, feature],\n",
    "                 hist=True,\n",
    "                 kde=True,\n",
    "                 ax=ax1,\n",
    "                 color='#e74c3c')\n",
    "    ax1.set(ylabel='Frequency')\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "\n",
    "    # Customizing the ecdf_plot.\n",
    "    ax2 = fig.add_subplot(grid[1, :2])\n",
    "    # Set the title.\n",
    "    ax2.set_title('Empirical CDF')\n",
    "    # Plotting the ecdf_Plot.\n",
    "    sns.distplot(df.loc[:, feature],\n",
    "                 ax=ax2,\n",
    "                 kde_kws={'cumulative': True},\n",
    "                 hist_kws={'cumulative': True},\n",
    "                 color='#e74c3c')\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n",
    "    ax2.set(ylabel='Cumulative Probability')\n",
    "\n",
    "    # Customizing the Box Plot.\n",
    "    ax3 = fig.add_subplot(grid[:, 2])\n",
    "    # Set title.\n",
    "    ax3.set_title('Box Plot')\n",
    "    # Plotting the box plot.\n",
    "    sns.boxplot(x=feature, data=df, orient='v', ax=ax3, color='#e74c3c')\n",
    "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n",
    "\n",
    "    plt.suptitle(f'{title}', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist3(trainv[trainv['crowded'] == 1], 'Character Count',\n",
    "           'Characters Per Event Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist3(trainv[trainv['crowded'] == 0], 'Character Count',\n",
    "           'Characters Per No Event Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHKBsQWg6r8-",
    "outputId": "551064dd-d02e-49df-9ecb-4f2f707ef536"
   },
   "outputs": [],
   "source": [
    "plot_dist3(trainv[trainv['Investigation'] == 'museumplein-druktebeeld'], 'Character Count',\n",
    "           'Characters Per \"Museumplein\" Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqpMgC8J6r9D",
    "outputId": "8de90e69-44e6-44e3-9252-95c0145dc14f"
   },
   "outputs": [],
   "source": [
    "plot_dist3(trainv[trainv['Investigation'] == 'dedam-druktebeeld'], 'Character Count',\n",
    "           'Characters Per \"Dam Square\" Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist3(trainv[trainv['Investigation'] == 'vondelpark-druktebeeld'], 'Character Count',\n",
    "           'Characters Per \"Vondelpark\" Tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Word_Counts\"></a>\n",
    "## Word Counts\n",
    "\n",
    "#### Ok let's check number of words per tweet now, they both look somewhat normally distributed, again disaster tweets seems to have slightly more words than non disaster ones. We might dig this deeper to get some more info in next part...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0Qc4ZDf6r9S"
   },
   "outputs": [],
   "source": [
    "def plot_word_number_histogram(plot1, plot2, plot3):\n",
    "    \n",
    "    \"\"\"A function for comparing word counts\"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(18, 6), sharey=True)\n",
    "    sns.distplot(plot1.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n",
    "    sns.distplot(plot2.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n",
    "    sns.distplot(plot3.str.split().map(lambda x: len(x)), ax=axes[2], color='#e74c3c')\n",
    "    \n",
    "    axes[0].set_xlabel('Word Count')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Vondelpark')\n",
    "    axes[1].set_xlabel('Word Count')\n",
    "    axes[1].set_title('Museumplein')\n",
    "    axes[2].set_xlabel('Word Count')\n",
    "    axes[2].set_title('De Dam')\n",
    "\n",
    "    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n",
    "    \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIXA_5mZ6r9V",
    "outputId": "4378a49a-23e0-4943-bdb4-35a5f1540005"
   },
   "outputs": [],
   "source": [
    "plot_word_number_histogram(trainv[trainv['Investigation'] == 'vondelpark-druktebeeld']['text'],\n",
    "                           trainv[trainv['Investigation'] == 'museumplein-druktebeeld']['text'],\n",
    "                           trainv[trainv['Investigation'] == 'dedam-druktebeeld']['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Word_Lengths\"></a>\n",
    "## Word Lengths\n",
    "\n",
    "#### This time we're gonna check if word complexity differs from tweet class. It looks like disaster tweets has longer words than non disaster ones in general. It's pretty visible which is good sign, yet again we can only assume at this stage...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "YNSr-8OT6r9a"
   },
   "outputs": [],
   "source": [
    "def plot_word_len_histogram(textno, textye):\n",
    "    \n",
    "    \"\"\"A function for comparing average word length\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n",
    "    sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
    "        lambda x: np.mean(x)),\n",
    "                 ax=axes[0], color='#e74c3c')\n",
    "    sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n",
    "        lambda x: np.mean(x)),\n",
    "                 ax=axes[1], color='#e74c3c')\n",
    "    \n",
    "    axes[0].set_xlabel('Word Length')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('No Event Tweets')\n",
    "    axes[1].set_xlabel('Word Length')\n",
    "    axes[1].set_title('Event Tweets')\n",
    "    \n",
    "    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n",
    "    fig.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8zNhLBN6r9c",
    "outputId": "73daa842-d1d9-4984-af2e-7b7e20ac3d4d"
   },
   "outputs": [],
   "source": [
    "plot_word_len_histogram(trainv[trainv['crowded'] == 0]['text'],\n",
    "                        trainv[trainv['crowded'] == 1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh9DIL5T6r9i"
   },
   "outputs": [],
   "source": [
    "lis = [\n",
    "    trainv[trainv['crowded'] == 0]['lemma_str'],\n",
    "    trainv[trainv['crowded'] == 1]['lemma_str']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAAWpTVG6r9l",
    "outputId": "e6c015f2-0144-4d82-e1ca-fea52f43297f"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, j in zip(lis, axes):\n",
    "    try:\n",
    "        new = i.str.split()\n",
    "        new = new.values.tolist()\n",
    "        corpus = [word.lower() for i in new for word in i]\n",
    "        dic = defaultdict(int)\n",
    "        for word in corpus:\n",
    "            if word in stop:\n",
    "                dic[word] += 1\n",
    "\n",
    "        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        x, y = zip(*top)\n",
    "        df = pd.DataFrame([x, y]).T\n",
    "        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n",
    "        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n",
    "        plt.tight_layout()\n",
    "    except:\n",
    "        plt.close()\n",
    "        print('No stopwords left in texts.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Most_Common_Words\"></a>\n",
    "## Most Common Words\n",
    "\n",
    "#### It's time to move to words themselves instead of their quantitative features. We start with most common words in both classes. I'd say it's pretty obvious if it's from disaster tweets or not. Disaster tweets has words like fire, kill, bomb indicating disasters. Meanwhile non disaster ones looks like pretty generic.\n",
    "\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "k6ATaXOA6r9s",
    "outputId": "44fb4bc9-3ed6-4c80-aa4d-135fca3e50bf"
   },
   "outputs": [],
   "source": [
    "# Displaying most common words.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, j in zip(lis, axes):\n",
    "\n",
    "    new = i.str.split()\n",
    "    new = new.values.tolist()\n",
    "    corpus = [word for i in new for word in i]\n",
    "\n",
    "    counter = Counter(corpus)\n",
    "    most = counter.most_common()\n",
    "    x, y = [], []\n",
    "    for word, count in most[:30]:\n",
    "        if (word not in stop):\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "\n",
    "    sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
    "axes[0].set_title('No Event Tweets')\n",
    "\n",
    "axes[1].set_title('Event Tweets')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Word')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Word')\n",
    "\n",
    "fig.suptitle('Most Common Unigrams', fontsize=24, va='baseline')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHo6CEJR6r9x"
   },
   "outputs": [],
   "source": [
    "def ngrams(n, title):\n",
    "    \"\"\"A Function to plot most common ngrams\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, j in zip(lis, axes):\n",
    "\n",
    "        new = i.str.split()\n",
    "        new = new.values.tolist()\n",
    "        corpus = [word for i in new for word in i]\n",
    "\n",
    "        def _get_top_ngram(corpus, n=None):\n",
    "            #getting top ngrams\n",
    "            vec = CountVectorizer(ngram_range=(n, n),\n",
    "                                  max_df=0.9,\n",
    "                                  stop_words='english').fit(corpus)\n",
    "            bag_of_words = vec.transform(corpus)\n",
    "            sum_words = bag_of_words.sum(axis=0)\n",
    "            words_freq = [(word, sum_words[0, idx])\n",
    "                          for word, idx in vec.vocabulary_.items()]\n",
    "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "            return words_freq[:15]\n",
    "\n",
    "        top_n_bigrams = _get_top_ngram(i, n)[:15]\n",
    "        x, y = map(list, zip(*top_n_bigrams))\n",
    "        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n",
    "        \n",
    "        axes[0].set_title('Event Tweets')\n",
    "        axes[1].set_title('No Event Tweets')\n",
    "        axes[0].set_xlabel('Count')\n",
    "        axes[0].set_ylabel('Words')\n",
    "        axes[1].set_xlabel('Count')\n",
    "        axes[1].set_ylabel('Words')\n",
    "        fig.suptitle(title, fontsize=24, va='baseline')\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Most_Common_Bigrams\"></a>\n",
    "## Most Common Bigrams\n",
    "\n",
    "#### Let's have a look for bigrams this time, which they are sequences of adjacent two words. Again it's pretty obvious to seperate two classes if it's disaster related or not. There are some confusing bigrams in non disaster ones like  body bag, emergency service etc. which needs deeper research but we'll leave it here since we got what we looking for in general.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-U71sUb6r91",
    "outputId": "b45bf732-570c-4be6-fcca-4f81ae26a75d"
   },
   "outputs": [],
   "source": [
    "ngrams(2, 'Most Common Bigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Most_Common_Trigrams\"></a>\n",
    "## Most Common Trigrams\n",
    "\n",
    "#### Alright! Things are much clearer with sequences of 3 words. The confusing body bags were cross body bags (Who uses them in these days anyways!) which I found it pretty funny when I found the reason of the confusion. Anyways we can see disasters are highly seperable now from non disaster ones, which is great!\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vunRj2r46r94",
    "outputId": "50adb84a-87a7-4ad8-fefe-42f726fd4f2a"
   },
   "outputs": [],
   "source": [
    "ngrams(3, 'Most Common Trigrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Some_Extra_Analysis\"></a>\n",
    "# Some Extra Analysis\n",
    "\n",
    "#### In this part we gonna apply some other analysis tecniques to gain some more insights about our dataset let's see what we can get from them...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Determining_Topics\"></a>\n",
    "## Determining Topics\n",
    "\n",
    "#### We'll be using a method called Non-Negative Matrix Factorization (NMF) to see if we can get some defined topics out of our TF-IDF matrix, with this way TF-IDF will decrease impact of the high frequency words, so we might get more specific topics.\n",
    "\n",
    "#### When we inspect our top ten topics we might need to use little imagination to help us understand them. Well actually they are pretty seperable again, I'd say disaster topics are much more clearer to read, we can see the topics directly by looking at them, meanwhile non disaster ones are more personal topics...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "5fhfFe8-6r96",
    "outputId": "3d6983bd-5cf9-4554-b851-5c3c6f176660"
   },
   "outputs": [],
   "source": [
    "def display_topics(text, no_top_words, topic):\n",
    "    \n",
    "    \"\"\" A function for determining the topics present in our corpus with nmf \"\"\"\n",
    "    \n",
    "    no_top_words = no_top_words\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=0.90, min_df=25, max_features=5000, use_idf=True)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(text)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    doc_term_matrix_tfidf = pd.DataFrame(\n",
    "        tfidf.toarray(), columns=list(tfidf_feature_names))\n",
    "    nmf = NMF(n_components=10, random_state=0,\n",
    "              alpha=.1, init='nndsvd').fit(tfidf)\n",
    "    print(topic)\n",
    "    for topic_idx, topic in enumerate(nmf.components_):\n",
    "        print('Topic %d:' % (topic_idx+1))\n",
    "        print(' '.join([tfidf_feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "\n",
    "display_topics(lis[0], 10, 'No Event Topics\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P12hIDKq6r-S",
    "outputId": "95631316-6af3-4f26-a89b-2590d2787cce"
   },
   "outputs": [],
   "source": [
    "display_topics(lis[1], 10,'Event Topics\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Word_Cloud\"></a>\n",
    "## Word Cloud\n",
    "\n",
    "#### Wordclouds are popular approach in NLP tasks. We're going to use the library exactly designed for it called \"WordCloud\", I also wanted to mask it with twitter logo shape and grey colors just for adding more interesting presentation and show what you can do with this library. When we look our word clouds we can clearly say which one is disaster on which one is not. Pretty good!\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RsFSbjT6r-W"
   },
   "outputs": [],
   "source": [
    "# Setting mask for wordcloud.\n",
    "\n",
    "mask = np.array(Image.open('/kaggle/input/twittermaskn/twittermask.png'))\n",
    "mask[mask.sum(axis=2) == 0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "g2Bq6qsM6r-Z"
   },
   "outputs": [],
   "source": [
    "def plot_wordcloud(text, title, title_size):\n",
    "    \"\"\" A function for creating wordcloud images \"\"\"\n",
    "    words = text\n",
    "    allwords = []\n",
    "    for wordlist in words:\n",
    "        allwords += wordlist\n",
    "    mostcommon = FreqDist(allwords).most_common(140)\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        background_color='black',\n",
    "        stopwords=set(STOPWORDS),\n",
    "        max_words=150,\n",
    "        scale=3,\n",
    "        mask=mask,\n",
    "        contour_width=0.1,\n",
    "        contour_color='grey',\n",
    "    ).generate(str(mostcommon))    \n",
    "\n",
    "    def grey_color_func(word,\n",
    "                        font_size,\n",
    "                        position,\n",
    "                        orientation,\n",
    "                        random_state=None,\n",
    "                        **kwargs):\n",
    "        # A definition for creating grey color shades.\n",
    "        return 'hsl(0, 0%%, %d%%)' % random.randint(60, 100)\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 18), facecolor='white')\n",
    "    plt.imshow(wordcloud.recolor(color_func=grey_color_func, random_state=42),\n",
    "               interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title,\n",
    "              fontdict={\n",
    "                  'size': title_size,\n",
    "                  'verticalalignment': 'bottom'\n",
    "              })\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86ToK4to6r-b",
    "outputId": "028adab6-9a56-40e9-fd7d-4a81e3532d50"
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(trainv[trainv['target'] == 0]['lemmatized'],\n",
    "               'Most Common Words in Non-Disaster Tweets',\n",
    "               title_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRb4QR7P6r-d",
    "outputId": "4049396c-8d3a-4d10-fa97-3d8d36ef8c94"
   },
   "outputs": [],
   "source": [
    "plot_wordcloud(trainv[trainv['target'] == 1]['lemmatized'],\n",
    "               'Most Common Words in Disaster Tweets',\n",
    "               title_size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Named_Entity_Recognition\"></a>\n",
    "## Named Entity Recognition\n",
    "\n",
    "#### One last thing before we move on the modelling is Named Entity Recognition. It's a method for extracting information from text and returns which entities that are present in the text are classified into predefined entity types like \"Person\", \"Place\", \"Organization\", etc. By using NER we can get great insights about the types of entities present in the given text dataset.\n",
    "\n",
    "#### When we look our NER results we can get lots of great insights. We can see that in disaster tweets countries, cities, states are much more common than non disaster ones. Again nationality or religious or political group names are more likely to be mentioned in disaster tweets. These are great indicators for us...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-KGYvFA6r-h"
   },
   "outputs": [],
   "source": [
    "# Loading NER.\n",
    "nlp = en_core_web_sm.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "s0P0Lj7Q6r-l"
   },
   "outputs": [],
   "source": [
    "def plot_named_entity_barchart(text):\n",
    "    \n",
    "    \"\"\"A function for extracting named entities and comparing them\"\"\"\n",
    "    \n",
    "    def _get_ner(text):\n",
    "        doc = nlp(text)\n",
    "        return [X.label_ for X in doc.ents]\n",
    "\n",
    "    ent = text.apply(lambda x: _get_ner(x))\n",
    "    ent = [x for sub in ent for x in sub]\n",
    "    counter = Counter(ent)\n",
    "    count = counter.most_common()\n",
    "\n",
    "    x, y = map(list, zip(*count))\n",
    "    sns.barplot(x=y, y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "ClLsxUro6r-o",
    "outputId": "ecd37dcb-cf76-46e7-cb91-c2190b20e53d"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "for i, j in zip(lis, axes):\n",
    "\n",
    "    def _get_ner(i):\n",
    "        doc = nlp(i)\n",
    "        return [X.label_ for X in doc.ents]\n",
    "\n",
    "    ent = i.apply(lambda x: _get_ner(x))\n",
    "    ent = [x for sub in ent for x in sub]\n",
    "    counter = Counter(ent)\n",
    "    count = counter.most_common()[:15]\n",
    "\n",
    "    x, y = map(list, zip(*count))\n",
    "    sns.barplot(x=y, y=x, ax=j, palette='plasma')\n",
    "\n",
    "axes[0].set_title('Non Disaster Tweets')\n",
    "axes[1].set_title('Disaster Tweets')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_ylabel('Named-Entity')\n",
    "axes[1].set_xlabel('Count')\n",
    "axes[1].set_ylabel('Named-Entity')\n",
    "fig.suptitle('Common Named-Entity Counts', fontsize=24, va='baseline')\n",
    "\n",
    "\n",
    "\n",
    "patch1 = mpatches.Patch(label='PERSON : People, including fictional')\n",
    "patch2 = mpatches.Patch(label='ORG : Companies, agencies, institutions, etc.')\n",
    "patch3 = mpatches.Patch(label='CARDINAL : Numerals that dont fall under another type.')\n",
    "patch4 = mpatches.Patch(label='GPE : Countries, cities, states.')\n",
    "patch5 = mpatches.Patch(label='NORP : Nationalities or religious or political groups.')\n",
    "patch6 = mpatches.Patch(label='TIME : Times smaller than a day.')\n",
    "patch7 = mpatches.Patch(label='QUANTITY : Measurements, as of weight or distance.')\n",
    "patch8 = mpatches.Patch(label='ORDINAL : “first”, “second”, etc.')\n",
    "patch9 = mpatches.Patch(label='LOC : Non-GPE locations, mountain ranges, bodies of water.')\n",
    "patch10 = mpatches.Patch(label='FAC : Buildings, airports, highways, bridges, etc.')\n",
    "patch11 = mpatches.Patch(label='PRODUCT : Objects, vehicles, foods, etc. (Not services.)')\n",
    "patch12 = mpatches.Patch(label='EVENT : Named hurricanes, battles, wars, sports events, etc.')\n",
    "patch13 = mpatches.Patch(label='LANGUAGE : Any named language.')\n",
    "patch14 = mpatches.Patch(label='PERCENT : Percentage, including ”%“.')\n",
    "patch15 = mpatches.Patch(label='DATE : Absolute or relative dates or periods.')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4, patch5, patch6, patch7, patch8, patch9, patch10, patch11, patch12, patch13, patch14, patch15, ],bbox_to_anchor=(1.05, 0.85), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Building_the_Model\"></a>\n",
    "# Building the Bert Model\n",
    "\n",
    "#### Finally it's time to start building our model. We gonna use BERT for this task with the help of library called Transformers which makes our work much more smoother...\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Getting_Things_Ready\"></a>\n",
    "## Getting Things Ready\n",
    "\n",
    "#### Let's start with setting our main computing device. The code below automatically choose GPU if it's available otherwise goes with CPU but I highly recommend GPU which is much faster. Then we load our traning and test data for modelling, as I said I'm not going to use our visualization data, BERT uses punctuations in modelling and as you'll see BERT does great job on raw text by itself.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyRYMKRQ6r-s",
    "outputId": "ff9b2087-0848-42b5-ebe7-17fc0e729783"
   },
   "outputs": [],
   "source": [
    "# If there's a GPU available...\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.  \n",
    "    \n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDH7qw3O6r-v",
    "outputId": "c35c82bd-0857-4962-9da9-d1e9bd32ddb1"
   },
   "outputs": [],
   "source": [
    "# Loading the data for modelling.\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "print(f'Number of training tweets: {train.shape[0]}\\n')\n",
    "print(f'Number of training tweets: {test.shape[0]}\\n')\n",
    "\n",
    "display(train.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmobtZpn6r-y"
   },
   "outputs": [],
   "source": [
    "# Setting target variables, creating combined data and saving index for dividing combined data later.\n",
    "\n",
    "labels = train['target'].values\n",
    "idx = len(labels)\n",
    "combined = pd.concat([train, test])\n",
    "combined = combined.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Tokenization_and_Formatting_the_Inputs\"></a>\n",
    "## Tokenization and Formatting the Inputs\n",
    "\n",
    "#### For feeding our text to BERT we have to tokenize our text first and then these tokens must be mapped. For this job we gonna download and use BERT's own tokenizer. Thanks to Transformers library it's like one line of code, we also convert our tokens to lowercase for uncased model. You can see how the tokenizer works below there on first row of tweets for example.\n",
    "\n",
    "#### We set our max len according to our tokenized sentences for padding and truncation, then we use tokenizer.encode_plus it'll split the sentences into tokens, then adds special tokens for classificication [CLS]: \n",
    "> The first token of every sequence is always a special classification token ([CLS]). The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. (from the BERT paper)\n",
    "#### Then it adds [SEP] tokens for making BERT decide if sentences are related. In our case it shouldn't be that important I think.\n",
    "\n",
    "#### Then our tokenizer map's our tokens to their IDs first and pads or truncates all sentences to same length according to our max length. If sentence is longer than our limit it gets truncated, if it's shorter than our defined length then it adds [PAD] tokens to get them in same length.\n",
    "\n",
    "#### Finally tokenizer create attention masks which is consisting of 1's and 0's for differentiating [PAD] tokens from the actual tokens.\n",
    "\n",
    "#### We do these steps for each train and test set and then get our converted data for our BERT model. We also split train test on our train data for checking our models accuracy. \n",
    "\n",
    "#### Lastly we define how to load the data into our model for training, since we can't use it all at once because of memory restrictions. On the official BERT paper batch size of 16 or 32 is recommended so we went with 32 since Kaggle offers us decent GPU's thanks to them!\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GFP3p2aA6r-1"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the combined text data using bert tokenizer.\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0_JPi_r6r-3",
    "outputId": "0aa56d6d-dab8-4de4-98dd-073548e3ffd5"
   },
   "outputs": [],
   "source": [
    "# Print the original tweet.\n",
    "\n",
    "print(' Original: ', combined[0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
    "\n",
    "# Print the sentence mapped to token ID's.\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap4apYT16r-4",
    "outputId": "ff808191-9a93-458f-e82f-a6f24eef749f"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "\n",
    "for text in combined:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    \n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of sentence lenghts:\n",
    "\n",
    "token_lens = []\n",
    "\n",
    "for text in combined:\n",
    "    tokens = tokenizer.encode(text, max_length = 512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying sentence length dist.\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 6))\n",
    "sns.distplot(token_lens, color='#e74c3c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I choose max len of 84 since it's the longest sentence we have here, playing with this number might get different results but the bigger you choose the slower the model will be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5nQqKaO6r-6",
    "outputId": "6bedb450-7dfb-4132-eda9-2283f0a27fb4"
   },
   "outputs": [],
   "source": [
    "# Splitting the train test data after tokenizing.\n",
    "\n",
    "train= combined[:idx]\n",
    "test = combined[idx:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqbud3--6r-8"
   },
   "outputs": [],
   "source": [
    "def tokenize_map(sentence,labs='None'):\n",
    "    \n",
    "    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in sentence:\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        \n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = 84,           # Max length according to our text data.\n",
    "                            pad_to_max_length = True, # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzUBnO5y6r--"
   },
   "outputs": [],
   "source": [
    "# Tokenizing all of the train test sentences and mapping the tokens to their word IDs.\n",
    "\n",
    "input_ids, attention_masks, labels = tokenize_map(train, labels)\n",
    "test_input_ids, test_attention_masks= tokenize_map(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AF-cP9956r-_",
    "outputId": "5a1e09ff-b85c-433a-a380-2146c38f30fb"
   },
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZL9--546r_C"
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rogeV1Nj6r_E"
   },
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Setting_the_Bert_Classification_Model\"></a>\n",
    "## Setting the Bert Classification Model\n",
    "\n",
    "#### It's time to load our model, exciting right?! Thanks to Transformers library we have exact tools we need for classification task. We set bert-large-uncased for the more accurate results and assign 2 labels for classification.\n",
    "\n",
    "#### You can see the model parameters down there, it's pretty straightforward with the transformers.\n",
    "\n",
    "#### Then we choose our optimizer and fine-tune our model. Again these hyperparameters (learning_rate, epsilon, epochs etc.) are recommended on the official BERT paper.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "id": "BityYQ8J6r_G",
    "outputId": "04838d37-af3e-4b47-82ef-be6e6a0a6719"
   },
   "outputs": [],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the device which we set GPU in our case.\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI49270K6r_I",
    "outputId": "d90f98b0-f706-423f-8189-c9e625edfb61"
   },
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples:\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7Qy4SHS6r_K"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch).\n",
    "\n",
    "# The 'W' stands for 'Weight Decay fix' probably...\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 6e-6, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2_EBwu96r_N"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "\n",
    "# We chose to run for 3, but we'll see later that this may be over-fitting the training data.\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs] (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Training_and_Evaluating\"></a>\n",
    "## Training and Evaluating\n",
    "\n",
    "#### Time to train our model! First we set some helper functions to calculate our metrics and time spent on the process. Then it moves like this, directly from the original notebook, it's pretty good at explaining I shouldn't confuse you with my own way of telling I guess:\n",
    "\n",
    "\n",
    "> Training:\n",
    "\n",
    ">-    Unpack our data inputs and labels\n",
    "-    Load data onto the GPU for acceleration,\n",
    "-    Clear out the gradients calculated in the previous pass,\n",
    "     -   In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out,\n",
    "-    Forward pass (feed input data through the network),\n",
    "-    Backward pass (backpropagation),\n",
    "-    Tell the network to update parameters with optimizer.step(),\n",
    "-    Track variables for monitoring progress.\n",
    "\n",
    ">Evalution:\n",
    "\n",
    ">-    Unpack our data inputs and labels,\n",
    "-    Load data onto the GPU for acceleration,\n",
    "-    Forward pass (feed input data through the network),\n",
    "-    Compute loss on our validation data and track variables for monitoring progress.\n",
    "\n",
    "Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line. \n",
    "\n",
    "\n",
    "The code below trains according to our data and saves the learning progress on the way so we can summarize at the end and see our results. We can also turn these to dataframe and plot it to see our eavluation better. So we can decide if the model performs well and not overfitting...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBO_0cQf6r_P"
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating accuracy scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "    \n",
    "    \"\"\"A function for calculating f1 scores\"\"\"\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return f1_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZc7gqT76r_R"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    \n",
    "    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n",
    "    \n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNBD02-66r_T",
    "outputId": "31ddb2d9-9f0e-4deb-97df-0e9ca81a1ef3"
   },
   "outputs": [],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print('')\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes:\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    \n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    \n",
    "    # `dropout` and `batchnorm` layers behave differently during training vs. test ,\n",
    "    # source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the device(gpu in our case) using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        \n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n",
    "        # Source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
    "        \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is down here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n",
    "        \n",
    "        # It returns different numbers of parameters depending on what arguments given and what flags are set. For our useage here, it returns the loss (because we provided labels),\n",
    "        # And the 'logits' (the model outputs prior to activation.)\n",
    "        \n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end, \n",
    "        # `loss` is a tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0 This is to help prevent the 'exploding gradients' problem.\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        \n",
    "        # The optimizer dictates the 'update rule'(How the parameters are modified based on their gradients, the learning rate, etc.)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epcoh took: {:}'.format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables:\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch.\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        \n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        \n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training part).\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is down here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers BertForSequenceClassification.\n",
    "            # Get the 'logits' output by the model. The 'logits' are the output values prior to applying an activation function like the softmax.\n",
    "            \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.2f}'.format(avg_val_f1))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    \n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Measure how long the validation run took:\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('')\n",
    "print('Training complete!')\n",
    "\n",
    "print('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UU10XeVz6r_V",
    "outputId": "85620a0b-c516-401b-e699-f4979b5440ac"
   },
   "outputs": [],
   "source": [
    "# Display floats with two decimal places.\n",
    "\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# Display the table.\n",
    "\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vybyY0tk6r_Y",
    "outputId": "29278bd4-e8e1-4253-8808-d756056658b5"
   },
   "outputs": [],
   "source": [
    "# Increase the plot size and font size:\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# Plot the learning curve:\n",
    "\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label='Training')\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label='Validation')\n",
    "\n",
    "# Label the plot:\n",
    "\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Predicting_and_Submission\"></a>\n",
    "## Predicting and Submission\n",
    "\n",
    "#### Ok we trained our model and it's ready to make predictions on our test data. Then we save them to csv file for submission, this part is pretty straightforward for the most of classification tasks.\n",
    "\n",
    "### [Back To Table of Contents](#top_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIkWerb86r_Z",
    "outputId": "7ab78938-a1bb-4daa-a9f9-93ef0a883d59"
   },
   "outputs": [],
   "source": [
    "# Prediction on test set:\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode:\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables :\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Predict:\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "    \n",
    "  # Add batch to GPU\n",
    "\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader:\n",
    "    \n",
    "  b_input_ids, b_input_mask, = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction:\n",
    "\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions:\n",
    "    \n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU:\n",
    "    \n",
    "  logits = logits.detach().cpu().numpy()\n",
    " \n",
    "  \n",
    "  # Store predictions and true labels:\n",
    "    \n",
    "  predictions.append(logits)\n",
    "\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBHLkwr06r_f"
   },
   "outputs": [],
   "source": [
    "# Getting list of predictions and then choosing the target value with using argmax on probabilities.\n",
    "\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DVmThnd6r_h",
    "outputId": "f7163ae0-e44d-4539-a302-1445a1d215c5"
   },
   "outputs": [],
   "source": [
    "# Creating submission data.\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n",
    "submission['target'] = flat_predictions\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXxu-Qcp6r_i"
   },
   "outputs": [],
   "source": [
    "# Saving submission to '.csv' file:\n",
    "\n",
    "submission.to_csv('submission.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well that's it then... Thank you all for reading! I hope it helps you on the way of learning NLP. I'm completely open to feedbacks for improving my work and learning more, I'm still a beginner and learning a lot every single day so please feel free to add your opinion in the comments.\n",
    "<div align='center'><font size=\"6\" color=\"#000000\"><b>And again please UPVOTE if you liked this notebook so it can reach more people, Thanks!</b></font></div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1812439174eb479e99d304086e6af7ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "18666d5f27294115b07c0942beb48594": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ac7faa59b2d847b3ba6600c266331fe8",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b1a7c31dfa8742a7a8d63f6c362f5aa0",
       "value": 231508
      }
     },
     "19916b25426242d7bc4e95e63296a1e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dcc46c9796e45668b186ab0acfbb2b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a0adb85125b491baa76de350995e88e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "66300c66cd3a454c9fc4a997eac2a70f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_810b01d3a3374475b34b7cca7234f44b",
       "placeholder": "​",
       "style": "IPY_MODEL_e8a478d9f5534b4ab20fdcc8c809273b",
       "value": " 1.34G/1.34G [00:38&lt;00:00, 35.1MB/s]"
      }
     },
     "72ab443668af4346b7c9ba1a702ec9fb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "798fcc11161b45ae8a4cfbc145b6b05d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7f7313047dc74114bc89e9e9ca4cfe5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "810b01d3a3374475b34b7cca7234f44b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "81b633ed81eb478e8b3b2c9d1caf8c89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a22aebe5e9e8403ebe9cc7aed690b3f2",
        "IPY_MODEL_fbe6b4ea1e3640dfab1a5cc314ba3dd9"
       ],
       "layout": "IPY_MODEL_19916b25426242d7bc4e95e63296a1e8"
      }
     },
     "8c8c0635a13a4fd9ab09bc2196e99f57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8067f4603c24d7f9cf354655683f69b",
       "max": 1344997306,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1812439174eb479e99d304086e6af7ad",
       "value": 1344997306
      }
     },
     "a213a4c026474fd5ad841f45bc8b5b4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18666d5f27294115b07c0942beb48594",
        "IPY_MODEL_b5c27118b2a54fd9902b6237fb2d11c6"
       ],
       "layout": "IPY_MODEL_f45cf18821ee4bae84d9fa3995ed80c3"
      }
     },
     "a22aebe5e9e8403ebe9cc7aed690b3f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3dcc46c9796e45668b186ab0acfbb2b3",
       "max": 434,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f7313047dc74114bc89e9e9ca4cfe5b",
       "value": 434
      }
     },
     "a8067f4603c24d7f9cf354655683f69b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac7faa59b2d847b3ba6600c266331fe8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1a7c31dfa8742a7a8d63f6c362f5aa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b5c27118b2a54fd9902b6237fb2d11c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ca203c350d6d4b2b87168c32b38d5c6c",
       "placeholder": "​",
       "style": "IPY_MODEL_798fcc11161b45ae8a4cfbc145b6b05d",
       "value": " 232k/232k [00:00&lt;00:00, 699kB/s]"
      }
     },
     "b85600f7e43240e7becec60ff1637c2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8c8c0635a13a4fd9ab09bc2196e99f57",
        "IPY_MODEL_66300c66cd3a454c9fc4a997eac2a70f"
       ],
       "layout": "IPY_MODEL_5a0adb85125b491baa76de350995e88e"
      }
     },
     "ca203c350d6d4b2b87168c32b38d5c6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e8a478d9f5534b4ab20fdcc8c809273b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ed711222d9cd41eeba1dbd4da231dfa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f45cf18821ee4bae84d9fa3995ed80c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fbe6b4ea1e3640dfab1a5cc314ba3dd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72ab443668af4346b7c9ba1a702ec9fb",
       "placeholder": "​",
       "style": "IPY_MODEL_ed711222d9cd41eeba1dbd4da231dfa0",
       "value": " 434/434 [00:00&lt;00:00, 4.96kB/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
